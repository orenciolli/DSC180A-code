{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: NLI\n",
    " - TODO: figure out ground truth retrieval/knowledge graph\n",
    " - considering using [k-BERT](https://arxiv.org/pdf/1909.07606.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: political Bias\n",
    "- use rbf svm with tf-idf\n",
    "- outputs: one class label, as an int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('liar_plus/train2.tsv', delimiter='\\t', header = None)\n",
    "df = df.drop(columns = [0])\n",
    "\n",
    "\n",
    "df.rename({1: 'id', 2: 'label', 3: 'statement', 4: 'subject', 5: 'speaker', 6: 'job-title',\n",
    "           7: 'state_info', 8: 'party_affiliation', 9: 'barely_true_counts', 10: 'false_counts',\n",
    "           11: 'half_true_counts', 12: 'mostly_true_counts', 13: 'pants_on_fire_counts', 14: 'context',\n",
    "           15: 'justification'\n",
    "          }, axis = 1, inplace = True)\n",
    "\n",
    "df = df[~df['statement'].isna()]\n",
    "\n",
    "\n",
    "uninformative = {'organization', 'newsmaker', 'activist', 'state-official', 'government-body',\n",
    "'journalist', 'columnist', 'talk-show-host', 'education-official', 'business-leader', \n",
    " 'Moderate', 'democratic-farmer-labor', 'ocean-state-tea-party-action' }\n",
    "\n",
    "df_bias = df[~df['party_affiliation'].isin(uninformative)]\n",
    "df_bias = df_bias[~df_bias['party_affiliation'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replace = {\n",
    "    'constitution-party': 0, 'libertarian': 1,\n",
    "    'tea-party-member': 2,\n",
    "    'republican': 3, 'none': 4, 'independent': 5,\n",
    "    'liberal-party-canada': 6, 'labor-leader': 7, \n",
    "    'democrat': 8, 'green': 9\n",
    "}\n",
    "\n",
    "df_bias['party_affiliation'] = df_bias['party_affiliation'].replace(to_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_bias['statement'], df_bias['party_affiliation']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(X, y, test_size=.2)\n",
    ")\n",
    "\n",
    "tfidf_bias = TfidfVectorizer()\n",
    "X_train = tfidf_bias.fit_transform(X_train)\n",
    "\n",
    "X_test = tfidf_bias.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, gamma=2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_model = SVC(gamma=2, C=1)\n",
    "bias_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Fallacies\n",
    "- outputs: one t/f label \n",
    "- TODO: turn to multiclass classification with specific logical fallacies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_fp = os.path.join(os.pardir, \"fallacy_detection\", \"cleaned_binary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = pd.read_csv(binary_fp)\n",
    "binary.replace({'Invalid': 0, 'Valid': 1}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = binary['input'], binary['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=.2)\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fallacy_model = AdaBoostClassifier()\n",
    "fallacy_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fallacy_model.predict(tfidf.transform(X_test)) == y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Model\" 4: NER + sentiment\n",
    "- Extract named entities, to be passed in to an embedding layer in final model\n",
    "- Compute sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/ociolli/.local/lib/python3.9/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ociolli/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ociolli/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/ociolli/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.22.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ociolli/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ociolli/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (49.6.0.post20210108)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ociolli/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/ociolli/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.26.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/ociolli/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ociolli/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/ociolli/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ociolli/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.61.2)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/ociolli/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ociolli/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/ociolli/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ociolli/.local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.1 in /home/ociolli/.local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ociolli/.local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ociolli/.local/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ociolli/.local/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.1)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/ociolli/.local/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(index = 1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "df['entities'] = df['statement'].apply(get_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "df['sentiment'] = df['statement'].apply(get_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Model: feedforward NN\n",
    "- Takes in the outputs of the previous models and outputs a single scalar representing the overall level of misinformation\n",
    "- The response variable (misinformation score) is simply the politifact rating mapped to integers 0-5\n",
    "- TODO: need to tune hyperparameters more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text):\n",
    "    \n",
    "    #bias\n",
    "    vec_text = tfidf_bias.transform([text])\n",
    "    bias = bias_model.predict(vec_text)[0]\n",
    "    \n",
    "    #fallacies\n",
    "    fall_vec = tfidf.transform([text])\n",
    "    fallacy = fallacy_model.predict(fall_vec)[0]\n",
    "    \n",
    "    #sentiment\n",
    "    sentiment = get_sentiment(text)\n",
    "    \n",
    "    entities = get_entities(text)\n",
    "    \n",
    "    \n",
    "    return np.array([bias, fallacy, sentiment,\n",
    "                     entities], dtype = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, entity_embedding_dim, vocab_size):\n",
    "        super(MetaModel, self).__init__()\n",
    "\n",
    "        # Embedding layer for named entities\n",
    "        self.entity_embedding = nn.EmbeddingBag(vocab_size, \n",
    "                                                entity_embedding_dim,\n",
    "                                                sparse=True)  # Assuming a maximum of 10000 unique entities\n",
    "\n",
    "        # Fully connected layers for scalar inputs\n",
    "        self.fc_scalar1 = nn.Linear(3, 15)\n",
    "        self.fc_scalar2 = nn.Linear(15, 30)\n",
    "\n",
    "        # Fully connected layers for the combined features\n",
    "        self.fc_combined1 = nn.Linear(entity_embedding_dim + 30, 20)\n",
    "        self.fc_combined2 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, scalar_inputs, entity_input):\n",
    "        \n",
    "        x_entity = self.entity_embedding(entity_input)\n",
    "\n",
    "        x_scalar = F.relu(self.fc_scalar1(scalar_inputs))\n",
    "        x_scalar = F.relu(self.fc_scalar2(x_scalar))\n",
    "\n",
    "        # Combine scalar and entity features\n",
    "        x_combined = torch.cat((x_entity, x_scalar), dim=1)\n",
    "\n",
    "        x_combined = F.relu(self.fc_combined1(x_combined))\n",
    "        x_combined = self.fc_combined2(x_combined)\n",
    "\n",
    "        return x_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'pants-fire': 5, 'false': 4, 'half-true': 3, \n",
    "             'barely-true': 2, 'mostly-true': 1, 'true': 0}\n",
    "\n",
    "liar_rating = df['label'].replace(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_cats = len(['CARDINAL', 'DATE', 'EVENT', \n",
    "'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', \n",
    " 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_statement = df['statement'].apply(vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(vector_statement, liar_rating, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 2.7415\n",
      "Epoch 2/5, Loss: 2.4176\n",
      "Epoch 3/5, Loss: 2.3896\n",
      "Epoch 4/5, Loss: 2.3482\n",
      "Epoch 5/5, Loss: 2.2729\n"
     ]
    }
   ],
   "source": [
    "X_train_numerical = torch.tensor(X_train.str[:3].values.tolist(), dtype=torch.float32)\n",
    "\n",
    "vocab_size = 10000\n",
    "\n",
    "entities = X_train.str[-1].values.tolist()\n",
    "\n",
    "entity_to_index_mapping = {entity: idx + 1 for idx, \n",
    "                           entity in enumerate(set(entity for entity_list in entities for entity in entity_list))}\n",
    "\n",
    "entity_inputs = pad_sequence([torch.tensor([entity_to_index_mapping.get(word, 0) for \n",
    "                                            word in entity_list]) \n",
    "                              for entity_list in entities], batch_first=True)\n",
    "\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "model = MetaModel(entity_embedding_dim=5, vocab_size=vocab_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Separate parameters for the embedding layer and other parameters\n",
    "embedding_params = list(model.entity_embedding.parameters())\n",
    "other_params = [param for name, param in model.named_parameters() if\n",
    "                not any(embedding_name in name for embedding_name in [\"entity_embedding\"])]\n",
    "\n",
    "# Separate optimizers for each set of parameters\n",
    "optimizer_embedding = optim.SparseAdam(embedding_params, lr=2e-1)\n",
    "optimizer_other = optim.Adam(other_params, lr=2e-3)\n",
    "\n",
    "train_data = TensorDataset(X_train_numerical, entity_inputs, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for numerical_inputs, entities, labels in train_loader:\n",
    "        optimizer_embedding.zero_grad()\n",
    "        optimizer_other.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(numerical_inputs, entities)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        # Update embedding parameters\n",
    "        optimizer_embedding.step()\n",
    "\n",
    "        # Update other parameters\n",
    "        optimizer_other.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_numerical = torch.tensor(X_test.str[:3].values.tolist(), dtype=torch.float32)\n",
    "\n",
    "entities_test = X_test.str[-1].values.tolist()\n",
    "\n",
    "\n",
    "entity_inputs_test = pad_sequence([torch.tensor([entity_to_index_mapping.get(word, 0) for \n",
    "                                            word in entity_list]) \n",
    "                              for entity_list in entities_test], batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_values = model(X_test_numerical, entity_inputs_test).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4638700959097575"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(predicted_values, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not good at predicting the \"very false\" labels! no 4s or 5s\n",
    "(predicted_values >= 4).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test >= 4).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue with fallacies \n",
    "- Not all claims have a logical structure, so it doesn't make sense to use fallacy classification for all of them\n",
    "- Probably should use in conjunction with a veracity prediction, not as a part of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacy_liar = (fallacy_model.predict(tfidf.transform(df['statement'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacy_liar_df = df.copy()\n",
    "fallacy_liar_df['fallacy'] = fallacy_liar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statement: If you look at states that are right to work, they constantly do not have budget deficits and they have very good business climates.\n",
      "\n",
      "justification: They did generally rank right-to-work states higher than union states, but they considered many other factors, including tax policy, that could account for the better rankings. (Get updates from PolitiFactRI on Twitter.\n",
      "\n",
      "label: barely-true\n"
     ]
    }
   ],
   "source": [
    "#good example: logical structure which is (arguably) fallacious\n",
    "print(\"statement: \" + fallacy_liar_df[fallacy_liar_df['fallacy'] == 1]['statement'].iloc[2] + '\\n') \n",
    "\n",
    "print( \n",
    "    \"justification: \" + fallacy_liar_df[fallacy_liar_df['fallacy'] == 1]['justification'].iloc[2] + '\\n'\n",
    ")\n",
    "\n",
    "print( \"label: \" + fallacy_liar_df[fallacy_liar_df['fallacy'] == 1]['label'].iloc[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statement: ISIS supporter tweeted at 10:34 a.m. Shooting began at 10:45 a.m. in Chattanooga, Tenn.\n",
      "\n",
      "justification: Geller updated the original post, but it did little to stuff the rumor back into the bag.\n",
      "\n",
      "label: false\n"
     ]
    }
   ],
   "source": [
    "#bad example: no logical structure, simple claim\n",
    "print(\"statement: \" + fallacy_liar_df[fallacy_liar_df['fallacy'] == 1]['statement'].iloc[1] + '\\n') \n",
    "\n",
    "print( \n",
    "    \"justification: \" + fallacy_liar_df[fallacy_liar_df['fallacy'] == 1]['justification'].iloc[1] + '\\n'\n",
    ")\n",
    "\n",
    "print( \"label: \" + fallacy_liar_df[fallacy_liar_df['fallacy'] == 1]['label'].iloc[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}