{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b91c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# import spacy\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91f07241",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.tsv', delimiter='\\t', header = None)\n",
    "\n",
    "df.rename({0: 'id', 1: 'label', 2: 'statement', 3: 'subject', 4: 'speaker', 5: 'job-title',\n",
    "           6: 'state_info', 7: 'party_affiliation', 8: 'barely_true_counts', 9: 'false_counts',\n",
    "           10: 'half_true_counts', 11: 'mostly_true_counts', 12: 'pants_on_fire_counts', 13: 'context'\n",
    "          }, axis = 1, inplace = True)\n",
    "\n",
    "mapping = {'false': 0, 'half-true': 1, 'mostly-true': 2, 'true': 3, 'barely-true': 4,\n",
    "       'pants-fire': 5}\n",
    "\n",
    "df['label'] = df['label'].replace(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c853f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['subject'] = label_encoder.fit_transform(df['subject'])\n",
    "\n",
    "party_encoder = LabelEncoder()\n",
    "df['party_affiliation'] = party_encoder.fit_transform(df['party_affiliation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15261e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return (blob.sentiment.polarity + 1) / 2\n",
    "\n",
    "df['sentiment'] = df['statement'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cd521d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ociolli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#removing stop words, neglecting casing\n",
    "\n",
    "df['statement'] = df['statement'].str.lower()\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords dataset\n",
    "nltk.download('stopwords')\n",
    "def remove_stopwords(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    filtered_sentence = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "df['statement_cleaned'] = df['statement'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e977c2",
   "metadata": {},
   "source": [
    "## Credibility Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e54e6eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1454/3898473581.py:17: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cs_lookup[speaker] = cs / (MTC + HTC + BTC + FC + PFC)\n"
     ]
    }
   ],
   "source": [
    "cs_lookup = {}\n",
    "\n",
    "for speaker in set(df['speaker'].unique()) - {np.nan}:\n",
    "    try:\n",
    "        limited = df[df['speaker'] == speaker]\n",
    "        vals = limited.iloc[0]\n",
    "       \n",
    "        MTC = vals.loc['mostly_true_counts']\n",
    "        HTC = vals.loc['half_true_counts']\n",
    "        BTC = vals.loc['barely_true_counts']\n",
    "        FC = vals.loc['false_counts']\n",
    "        PFC = vals.loc['pants_on_fire_counts']\n",
    "\n",
    "        #exclude true counts = weight 0\n",
    "        cs = (0.2 * MTC) + (0.5 * HTC) + (0.75 * BTC) + (0.9 * FC) + (1 * PFC)\n",
    "\n",
    "        cs_lookup[speaker] = cs / (MTC + HTC + BTC + FC + PFC)\n",
    "        \n",
    "    except:\n",
    "        print(speaker)\n",
    "\n",
    "mean_cs = np.nanmean(list(cs_lookup.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad886e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in cs_lookup.items():\n",
    "    if np.isnan(val):\n",
    "        cs_lookup[key] = mean_cs\n",
    "    else: continue\n",
    "        \n",
    "\n",
    "def impute_cred(val):\n",
    "    if not isinstance(val, str) or (val not in cs_lookup.keys()):\n",
    "        return mean_cs\n",
    "\n",
    "    else:\n",
    "        return cs_lookup[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e05bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['credibility'] = df['speaker'].apply(impute_cred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286272d2",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8288e8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('cuda')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3d67d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df):\n",
    "        self.encodings = tokenizer(df['statement'].tolist(), \n",
    "                                   truncation=True, padding='max_length', return_tensors='pt', max_length=768)\n",
    "        self.party_affiliations = torch.tensor(df['party_affiliation'].values)\n",
    "        self.credibilities = torch.tensor(df['credibility'].values)\n",
    "        self.sentiments = torch.tensor(df['sentiment'].values)\n",
    "        self.subjects = torch.tensor(df['subject'].values)\n",
    "\n",
    "        self.labels = torch.tensor(df['label'].values)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['party_affiliation'] = self.party_affiliations[idx]\n",
    "        item['credibility'] = self.credibilities[idx]\n",
    "        item['sentiment'] = self.sentiments[idx]\n",
    "        item['subject'] = self.subjects[idx]\n",
    "        item['label'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "train_dataset = CustomDataset(tokenizer, train_df)\n",
    "val_dataset = CustomDataset(tokenizer, val_df)\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e019c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_classes=6, num_heads=8, num_layers=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # Statement layers\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=768, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=hidden_size,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.transformer_layer, \n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(p=0.15)\n",
    "        self.state_fc = nn.Linear(768, 128)\n",
    "\n",
    "        # Feature layers\n",
    "        self.party_embed = torch.nn.Embedding(24, 32)\n",
    "        self.subject_embed = torch.nn.Embedding(3828, 256)\n",
    "\n",
    "        self.feature_fc = nn.Linear(290, 128)\n",
    "        self.dropout2 = torch.nn.Dropout(p=0.25)\n",
    "        \n",
    "        self.combined_fc_1 = nn.Linear(256, 64)\n",
    "        self.combined_fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, input_seq, party_affiliation, subject, credibility, sentiment):\n",
    "        # Statement branch\n",
    "        statement_out = self.transformer_encoder(input_seq.float()) \n",
    "        statement_out = self.dropout(statement_out)\n",
    "        statement_out = F.relu(self.state_fc(statement_out))\n",
    "        statement_out = self.dropout(statement_out)\n",
    "\n",
    "        # Feature branch\n",
    "        party_affiliation = self.party_embed(party_affiliation)\n",
    "        subject = self.subject_embed(subject)\n",
    "\n",
    "        feature_vec = torch.cat([party_affiliation.float(), subject.float(),\n",
    "                                 credibility.unsqueeze(1), sentiment.unsqueeze(1)], dim=1)\n",
    "        feature_out = F.relu(self.feature_fc(feature_vec.float()))\n",
    "\n",
    "    \n",
    "        combined = torch.cat([statement_out, feature_out], dim=1)\n",
    "        combined = self.combined_fc_1(combined)\n",
    "        combined = self.combined_fc(combined)\n",
    "\n",
    "        return F.softmax(combined, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "364f7a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Validation Accuracy: 0.2104\n",
      "Epoch 2/15, Validation Accuracy: 0.2241\n",
      "Epoch 3/15, Validation Accuracy: 0.2261\n",
      "Epoch 4/15, Validation Accuracy: 0.2300\n",
      "Epoch 5/15, Validation Accuracy: 0.2271\n",
      "Epoch 6/15, Validation Accuracy: 0.2354\n",
      "Epoch 7/15, Validation Accuracy: 0.2231\n",
      "Epoch 8/15, Validation Accuracy: 0.2271\n",
      "Epoch 9/15, Validation Accuracy: 0.2324\n",
      "Epoch 10/15, Validation Accuracy: 0.2275\n",
      "Epoch 11/15, Validation Accuracy: 0.2300\n",
      "Epoch 12/15, Validation Accuracy: 0.2261\n",
      "Epoch 13/15, Validation Accuracy: 0.2319\n",
      "Epoch 14/15, Validation Accuracy: 0.2363\n",
      "Epoch 15/15, Validation Accuracy: 0.2314\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_data_loader:\n",
    "\n",
    "        for key in batch:\n",
    "            batch[key] = batch[key].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(batch['input_ids'],\n",
    "                       batch['party_affiliation'],\n",
    "                       batch['subject'], \n",
    "                       batch['credibility'], \n",
    "                       batch['sentiment'])\n",
    "\n",
    "        loss = criterion(output, batch['label'])\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = []\n",
    "        val_labels = []\n",
    "\n",
    "        \n",
    "        for val_batch in val_data_loader:\n",
    "\n",
    "            for key in val_batch:\n",
    "                val_batch[key] = val_batch[key].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            val_output = model(val_batch['input_ids'],\n",
    "                                val_batch['party_affiliation'],\n",
    "                                val_batch['subject'],\n",
    "                                val_batch['credibility'],\n",
    "                                val_batch['sentiment'])\n",
    "            val_outputs.append(val_output)\n",
    "            val_labels.append(val_batch['label'])\n",
    "\n",
    "        val_outputs = torch.cat(val_outputs, dim=0)\n",
    "        val_labels = torch.cat(val_labels, dim=0)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        _, predicted = torch.max(val_outputs, 1)\n",
    "        val_accuracy = accuracy_score(val_labels.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77486f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
