{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc97ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c98a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8544df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# import spacy\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9eef39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.tsv', delimiter='\\t', header = None)\n",
    "\n",
    "df.rename({0: 'id', 1: 'label', 2: 'statement', 3: 'subject', 4: 'speaker', 5: 'job-title',\n",
    "           6: 'state_info', 7: 'party_affiliation', 8: 'barely_true_counts', 9: 'false_counts',\n",
    "           10: 'half_true_counts', 11: 'mostly_true_counts', 12: 'pants_on_fire_counts', 13: 'context'\n",
    "          }, axis = 1, inplace = True)\n",
    "\n",
    "mapping = {'false': 0, 'half-true': 1, 'mostly-true': 2, 'true': 3, 'barely-true': 4,\n",
    "       'pants-fire': 5}\n",
    "\n",
    "df['label'] = df['label'].replace(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179229b9",
   "metadata": {},
   "source": [
    "# Data engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fc38508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_party(val):\n",
    "    \n",
    "    left = {'democrat', 'green', 'democratic-farmer-labor', 'ocean-state-tea-party-action'}\n",
    "    right = {'republican', 'libertarian', 'tea-party-member', 'Moderate',\n",
    "       'constitution-party'}\n",
    "    \n",
    "    center_none = {'none', 'organization', 'independent',\n",
    "       'columnist', 'activist', 'talk-show-host',\n",
    "       'newsmaker', 'journalist', 'labor-leader', 'state-official',\n",
    "       'business-leader', 'education-official', 'tea-party-member', np.NaN,\n",
    "       'liberal-party-canada', 'government-body', 'Moderate',\n",
    "       }\n",
    "    \n",
    "    if val in left:\n",
    "        return 0\n",
    "    elif val in center_none:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "#df['party_affiliation'] = df['party_affiliation'].apply(clean_party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9cf84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['subject'] = label_encoder.fit_transform(df['subject'])\n",
    "\n",
    "party_encoder = LabelEncoder()\n",
    "df['party_affiliation'] = party_encoder.fit_transform(df['party_affiliation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa3f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return (blob.sentiment.polarity + 1) / 2\n",
    "\n",
    "df['sentiment'] = df['statement'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e3ebd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ociolli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#removing stop words, neglecting casing\n",
    "\n",
    "df['statement'] = df['statement'].str.lower()\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords dataset\n",
    "nltk.download('stopwords')\n",
    "def remove_stopwords(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    filtered_sentence = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "df['statement_cleaned'] = df['statement'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416965c",
   "metadata": {},
   "source": [
    "# Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55419f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_149/609049260.py:11: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, word2vec_file)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "\n",
    "# Path to the downloaded GloVe embeddings file\n",
    "glove_file = 'glove.6B.100d.txt'\n",
    "\n",
    "word2vec_file = 'glove.6B.100d.word2vec'  # Any path and filename you prefer\n",
    "\n",
    "# Convert GloVe format to Word2Vec format\n",
    "glove2word2vec(glove_file, word2vec_file)\n",
    "\n",
    "# Load the Word2Vec model\n",
    "glove_model = KeyedVectors.load_word2vec_format(word2vec_file, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ef5a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence, model, dim=100):\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Filter out words that are not in the model's vocabulary\n",
    "    words_in_vocab = [word for word in words if word in model.key_to_index]\n",
    "    \n",
    "    # Check if there are words in the sentence that are in the model's vocabulary\n",
    "    if words_in_vocab:\n",
    "        # Compute the mean of word embeddings for the words in the sentence\n",
    "        embedding = sum(model[word] for word in words_in_vocab) / len(words_in_vocab)\n",
    "        return embedding\n",
    "    else:\n",
    "        # If none of the words in the sentence are in the model's vocabulary, return None\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c5c846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['glove'] = df['statement_cleaned'].apply(lambda x: sentence_embedding(x, glove_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a0465",
   "metadata": {},
   "source": [
    "# Credibility score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d311dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3129/2228308207.py:17: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cs_lookup[speaker] = cs / (MTC + HTC + BTC + FC + PFC)\n"
     ]
    }
   ],
   "source": [
    "cs_lookup = {}\n",
    "\n",
    "for speaker in set(df['speaker'].unique()) - {np.nan}:\n",
    "    try:\n",
    "        limited = df[df['speaker'] == speaker]\n",
    "        vals = limited.iloc[0]\n",
    "       \n",
    "        MTC = vals.loc['mostly_true_counts']\n",
    "        HTC = vals.loc['half_true_counts']\n",
    "        BTC = vals.loc['barely_true_counts']\n",
    "        FC = vals.loc['false_counts']\n",
    "        PFC = vals.loc['pants_on_fire_counts']\n",
    "\n",
    "        #exclude true counts = weight 0\n",
    "        cs = (0.2 * MTC) + (0.5 * HTC) + (0.75 * BTC) + (0.9 * FC) + (1 * PFC)\n",
    "\n",
    "        cs_lookup[speaker] = cs / (MTC + HTC + BTC + FC + PFC)\n",
    "        \n",
    "    except:\n",
    "        print(speaker)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab653d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cs = np.nanmean(list(cs_lookup.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2a326f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in cs_lookup.items():\n",
    "    if np.isnan(val):\n",
    "        cs_lookup[key] = mean_cs\n",
    "    else: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de6bfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_cred(val):\n",
    "    if not isinstance(val, str) or (val not in cs_lookup.keys()):\n",
    "        return mean_cs\n",
    "\n",
    "    else:\n",
    "        return cs_lookup[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2004111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['credibility'] = df['speaker'].apply(impute_cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8da949de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('cuda')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcbcc46",
   "metadata": {},
   "source": [
    "# Transformer Model (incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ec913ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df):\n",
    "        self.encodings = tokenizer(df['statement'].tolist(), \n",
    "                                   truncation=True, padding=True, return_tensors='pt', max_length=256)\n",
    "        self.party_affiliations = torch.tensor(df['party_affiliation'].values)\n",
    "        self.credibilities = torch.tensor(df['credibility'].values)\n",
    "        self.sentiments = torch.tensor(df['sentiment'].values)\n",
    "        self.subjects = torch.tensor(df['subject'].values)\n",
    "\n",
    "        self.labels = torch.tensor(df['label'].values)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['party_affiliation'] = self.party_affiliations[idx]\n",
    "        item['credibility'] = self.credibilities[idx]\n",
    "        item['sentiment'] = self.sentiments[idx]\n",
    "        item['subject'] = self.subjects[idx]\n",
    "        item['label'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(tokenizer, train_df)\n",
    "val_dataset = CustomDataset(tokenizer, val_df)\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fd30211",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_classes=6, num_heads=8, num_layers=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # Statement layers\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=256, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=hidden_size,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.transformer_layer, \n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(p=0.15)\n",
    "        self.state_fc = nn.Linear(256, 64)\n",
    "\n",
    "        # Feature layers\n",
    "        self.party_embed = torch.nn.Embedding(24, 32)\n",
    "        self.subject_embed = torch.nn.Embedding(3828, 128)\n",
    "\n",
    "        self.feature_fc = nn.Linear(162, 128)\n",
    "        self.dropout2 = torch.nn.Dropout(p=0.25)\n",
    "\n",
    "        self.combined_fc = nn.Linear(192, 6)\n",
    "\n",
    "    def forward(self, input_seq, party_affiliation, subject, credibility, sentiment):\n",
    "        # Statement branch\n",
    "        statement_out = self.transformer_encoder(input_seq.float()) \n",
    "        statement_out = statement_out.squeeze(dim=0)\n",
    "        statement_out = self.dropout(statement_out)\n",
    "        statement_out = F.relu(self.state_fc(statement_out))\n",
    "        statement_out = self.dropout(statement_out)\n",
    "\n",
    "        # Feature branch\n",
    "        party_affiliation = self.party_embed(party_affiliation)\n",
    "        subject = self.subject_embed(subject)\n",
    "\n",
    "        feature_vec = torch.cat([party_affiliation.float(), subject.float(),\n",
    "                                 credibility.unsqueeze(1), sentiment.unsqueeze(1)], dim=1)\n",
    "        feature_out = F.relu(self.feature_fc(feature_vec.float()))\n",
    "\n",
    "    \n",
    "        combined = torch.cat([statement_out, feature_out], dim=1)\n",
    "        combined = self.combined_fc(combined)\n",
    "\n",
    "        return F.softmax(combined, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0e2b5",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c99ef72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "610ce4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc7a0e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n",
      "Input sequence shape: 256\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "was expecting embedding dimension of 256, but got 96",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3129/2608634028.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             val_output = model(val_batch['input_ids'],  # Assuming 'input_ids' is the key for BERT embeddings\n\u001b[0m\u001b[1;32m     46\u001b[0m                                 \u001b[0mval_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'party_affiliation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                                 \u001b[0mval_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3129/1746317044.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, party_affiliation, subject, credibility, sentiment)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Statement branch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#         input_seq = input_seq.permute(1, 0, 2)  # Change the input shape for the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mstatement_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mstatement_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatement_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mstatement_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask_for_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_nested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    597\u001b[0m     def _sa_block(self, x: Tensor,\n\u001b[1;32m    598\u001b[0m                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n\u001b[0;32m--> 599\u001b[0;31m         x = self.self_attn(x, x, x,\n\u001b[0m\u001b[1;32m    600\u001b[0m                            \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                            \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1203\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1206\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5202\u001b[0m             \u001b[0mis_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5204\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0membed_dim_to_check\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5205\u001b[0m         \u001b[0;34mf\"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5206\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: was expecting embedding dimension of 256, but got 96"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_data_loader:\n",
    "\n",
    "        for key in batch:\n",
    "            batch[key] = batch[key].to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        print(\"Input sequence shape:\", batch['input_ids'].size(-1))\n",
    "\n",
    "        output = model(batch['input_ids'],  # Assuming 'input_ids' is the key for BERT embeddings\n",
    "                       batch['party_affiliation'],\n",
    "                       batch['subject'], \n",
    "                       batch['credibility'], \n",
    "                       batch['sentiment'])\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, batch['label'])\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        \n",
    "#         has_nan = any(torch.isnan(param.grad).any() for param in model.parameters())\n",
    "#         if has_nan:\n",
    "#             print(\"NaN gradient detected before backward pass.\")\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = []\n",
    "        val_labels = []\n",
    "\n",
    "        for val_batch in val_data_loader:\n",
    "\n",
    "            for key in val_batch:\n",
    "                val_batch[key] = val_batch[key].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            val_output = model(val_batch['input_ids'],  # Assuming 'input_ids' is the key for BERT embeddings\n",
    "                                val_batch['party_affiliation'],\n",
    "                                val_batch['subject'],\n",
    "                                val_batch['credibility'],\n",
    "                                val_batch['sentiment'])\n",
    "            val_outputs.append(val_output)\n",
    "            val_labels.append(val_batch['label'])\n",
    "\n",
    "        val_outputs = torch.cat(val_outputs, dim=0)\n",
    "        val_labels = torch.cat(val_labels, dim=0)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        _, predicted = torch.max(val_outputs, 1)\n",
    "        val_accuracy = accuracy_score(val_labels.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c3eb8",
   "metadata": {},
   "source": [
    "# Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad0236ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('cuda')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e4538c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment, party_affiliation (subject later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76c0c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class seqModel(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_classes=6):\n",
    "        super(seqModel, self).__init__()\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(100, hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(p = 0.15)\n",
    "        self.state_fc = nn.Linear(hidden_size, 64)\n",
    "\n",
    "        self.feature_fc = nn.Linear(3, 64)\n",
    "\n",
    "        self.combined_fc = nn.Linear(128, 6)\n",
    "\n",
    "            \n",
    "    def forward(self, input_seq, party_affiliation, credibility, sentiment):\n",
    "        \n",
    "        # statement branch\n",
    "        statement_out, _ = self.lstm(input_seq)\n",
    "#         statement_out = statement_out[:, -1, :]\n",
    "        statement_out = self.dropout(statement_out)\n",
    "        statement_out = F.relu(self.state_fc(statement_out))\n",
    "        statement_out = self.dropout(statement_out)\n",
    "\n",
    "        # feature branch\n",
    "        feature_vec = torch.cat([party_affiliation.unsqueeze(1),\n",
    "                                 credibility.unsqueeze(1), sentiment.unsqueeze(1)], dim = 1)\n",
    "        \n",
    "        feature_out = F.relu(self.feature_fc(feature_vec))\n",
    "\n",
    "        combined = torch.cat([statement_out, feature_out], dim = 1)\n",
    "        combined = self.combined_fc(combined)\n",
    "\n",
    "        return F.softmax(combined, dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2a7b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = torch.tensor(df['label'].values, dtype=torch.long)\n",
    "        self.glove_embeddings = torch.stack([torch.tensor(embedding) for embedding in df['glove']], dim=0)\n",
    "        self.party_affiliation = torch.tensor(df['party_affiliation'].values, dtype=torch.long)\n",
    "        self.credibility = torch.tensor(df['credibility'].values, dtype=torch.float)\n",
    "        self.sentiment = torch.tensor(df['sentiment'].values, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'label': self.labels[idx],\n",
    "            'glove_embedding': self.glove_embeddings[idx],\n",
    "            'party_affiliation': self.party_affiliation[idx],\n",
    "            'credibility': self.credibility[idx],\n",
    "            'sentiment': self.sentiment[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9102cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(train_df)\n",
    "val_dataset = CustomDataset(val_df)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab8c81",
   "metadata": {},
   "source": [
    "# Training and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa0e284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seqModel()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad598276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Validation Accuracy: 0.2290\n",
      "Epoch 2/30, Validation Accuracy: 0.2607\n",
      "Epoch 3/30, Validation Accuracy: 0.2729\n",
      "Epoch 4/30, Validation Accuracy: 0.2856\n",
      "Epoch 5/30, Validation Accuracy: 0.2988\n",
      "Epoch 6/30, Validation Accuracy: 0.3120\n",
      "Epoch 7/30, Validation Accuracy: 0.2969\n",
      "Epoch 8/30, Validation Accuracy: 0.3091\n",
      "Epoch 9/30, Validation Accuracy: 0.3052\n",
      "Epoch 10/30, Validation Accuracy: 0.3262\n",
      "Epoch 11/30, Validation Accuracy: 0.3286\n",
      "Epoch 12/30, Validation Accuracy: 0.3325\n",
      "Epoch 13/30, Validation Accuracy: 0.3159\n",
      "Epoch 14/30, Validation Accuracy: 0.3306\n",
      "Epoch 15/30, Validation Accuracy: 0.3291\n",
      "Epoch 16/30, Validation Accuracy: 0.3267\n",
      "Epoch 17/30, Validation Accuracy: 0.3252\n",
      "Epoch 18/30, Validation Accuracy: 0.3174\n",
      "Epoch 19/30, Validation Accuracy: 0.3242\n",
      "Epoch 20/30, Validation Accuracy: 0.3262\n",
      "Epoch 21/30, Validation Accuracy: 0.3291\n",
      "Epoch 22/30, Validation Accuracy: 0.3271\n",
      "Epoch 23/30, Validation Accuracy: 0.3306\n",
      "Epoch 24/30, Validation Accuracy: 0.3247\n",
      "Epoch 25/30, Validation Accuracy: 0.3301\n",
      "Epoch 26/30, Validation Accuracy: 0.3320\n",
      "Epoch 27/30, Validation Accuracy: 0.3271\n",
      "Epoch 28/30, Validation Accuracy: 0.3301\n",
      "Epoch 29/30, Validation Accuracy: 0.3315\n",
      "Epoch 30/30, Validation Accuracy: 0.3369\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_data_loader:\n",
    "\n",
    "        for key in batch:\n",
    "            batch[key] = batch[key].to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(batch['glove_embedding'], batch['party_affiliation'],\n",
    "                       batch['credibility'], batch['sentiment'])\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, batch['label'])\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = []\n",
    "        val_labels = []\n",
    "\n",
    "        for val_batch in val_data_loader:\n",
    "            # Move data to device\n",
    "            for key in val_batch:\n",
    "                val_batch[key] = val_batch[key].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            val_output = model(val_batch['glove_embedding'], val_batch['party_affiliation'],\n",
    "                               val_batch['credibility'], val_batch['sentiment'])\n",
    "            val_outputs.append(val_output)\n",
    "            val_labels.append(val_batch['label'])\n",
    "\n",
    "        val_outputs = torch.cat(val_outputs, dim=0)\n",
    "        val_labels = torch.cat(val_labels, dim=0)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        _, predicted = torch.max(val_outputs, 1)\n",
    "        val_accuracy = accuracy_score(val_labels.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9abd59a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32421875\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = []\n",
    "    val_labels = []\n",
    "\n",
    "    for val_batch in val_data_loader:\n",
    "        # Move data to device\n",
    "        for key in val_batch:\n",
    "            val_batch[key] = val_batch[key].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        val_output = model(val_batch['glove_embedding'], val_batch['party_affiliation'],\n",
    "                           val_batch['credibility'], val_batch['sentiment'])\n",
    "        val_outputs.append(val_output)\n",
    "        val_labels.append(val_batch['label'])\n",
    "\n",
    "    val_outputs = torch.cat(val_outputs, dim=0)\n",
    "    val_labels = torch.cat(val_labels, dim=0)\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    _, predicted = torch.max(val_outputs, 1)\n",
    "    val_accuracy = accuracy_score(val_labels.cpu().numpy(), predicted.cpu().numpy())\n",
    "    print(val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4fa431",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a4569fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.tsv', delimiter='\\t', header = None)\n",
    "\n",
    "test.rename({0: 'id', 1: 'label', 2: 'statement', 3: 'subject', 4: 'speaker', 5: 'job-title',\n",
    "           6: 'state_info', 7: 'party_affiliation', 8: 'barely_true_counts', 9: 'false_counts',\n",
    "           10: 'half_true_counts', 11: 'mostly_true_counts', 12: 'pants_on_fire_counts', 13: 'context'\n",
    "          }, axis = 1, inplace = True)\n",
    "\n",
    "mapping = {'false': 0, 'half-true': 1, 'mostly-true': 2, 'true': 3, 'barely-true': 4,\n",
    "       'pants-fire': 5}\n",
    "\n",
    "test['label'] = test['label'].replace(mapping)\n",
    "\n",
    "\n",
    "test['party_affiliation'] = test['party_affiliation'].apply(clean_party)\n",
    "#test['subject'] = label_encoder.fit_transform(test['subject'])\n",
    "test['credibility'] = test['speaker'].apply(impute_cred)\n",
    "test['sentiment'] = test['statement'].apply(get_sentiment)\n",
    "\n",
    "test['statement'] = test['statement'].str.lower()\n",
    "test['statement_cleaned'] = test['statement'].apply(remove_stopwords)\n",
    "test['glove'] = test['statement_cleaned'].apply(lambda x: sentence_embedding(x, glove_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3cd2439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test)\n",
    "\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c824ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.287292817679558\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = []\n",
    "    test_labels = []\n",
    "\n",
    "    for test_batch in test_data_loader:\n",
    "        # Move data to device\n",
    "        for key in test_batch:\n",
    "            test_batch[key] = test_batch[key].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        test_output = model(test_batch['glove_embedding'], test_batch['party_affiliation'],\n",
    "                           test_batch['credibility'], test_batch['sentiment'])\n",
    "        test_outputs.append(test_output)\n",
    "        test_labels.append(test_batch['label'])\n",
    "\n",
    "    test_outputs = torch.cat(test_outputs, dim=0)\n",
    "    test_labels = torch.cat(test_labels, dim=0)\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    test_accuracy = accuracy_score(test_labels.cpu().numpy(), predicted.cpu().numpy())\n",
    "    print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3b59c",
   "metadata": {},
   "source": [
    "# Experiment with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d3f08ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee5ad700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, party_affiliations, credibilities, sentiments, labels):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt', max_length=256)\n",
    "        self.party_affiliations = torch.tensor(party_affiliations)\n",
    "        self.credibilities = torch.tensor(credibilities)\n",
    "        self.sentiments = torch.tensor(sentiments)\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['party_affiliation'] = self.party_affiliations[idx]\n",
    "        item['credibility'] = self.credibilities[idx]\n",
    "        item['sentiment'] = self.sentiments[idx]\n",
    "        item['label'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_texts, val_texts, train_party_affiliations, val_party_affiliations, \\\n",
    "train_credibilities, val_credibilities, train_sentiments, val_sentiments, \\\n",
    "train_labels, val_labels = train_test_split(\n",
    "    df['statement_cleaned'], df['party_affiliation'], df['credibility'], df['sentiment'], df['label'],\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = CustomDataset(tokenizer, train_texts.tolist(), train_party_affiliations.tolist(),\n",
    "                              train_credibilities.tolist(), train_sentiments.tolist(), train_labels.tolist())\n",
    "val_dataset = CustomDataset(tokenizer, val_texts.tolist(), val_party_affiliations.tolist(), \n",
    "                            val_credibilities.tolist(), val_sentiments.tolist(), val_labels.tolist())\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bec9dd87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05c6ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class seqBert(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_size=256, num_classes=6):\n",
    "        super(seqBert, self).__init__()\n",
    "        \n",
    "        self.bert_model = bert_model\n",
    "        \n",
    "#         self.lstm = torch.nn.LSTM(768, hidden_size) #new\n",
    "#         self.dropout = torch.nn.Dropout(p = 0.15) #new \n",
    "        self.state_fc = nn.Linear(hidden_size, 64) #new\n",
    "\n",
    "        # Feature branch\n",
    "        self.feature_fc = nn.Linear(3, 64)\n",
    "\n",
    "        # Combined branch\n",
    "        self.combined_fc = nn.Linear(bert_model.config.hidden_size + 64, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, party_affiliation, credibility, sentiment):\n",
    "        # BERT branch\n",
    "        bert_out = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)[0][:, 0, :]  # [CLS] token\n",
    "#         bert_out, _ = self.lstm(bert_out)\n",
    "        bert_out = self.dropout(bert_out)\n",
    "        bert_out = F.relu(self.state_fc(bert_out))\n",
    "        bert_out = self.dropout(bert_out)\n",
    "\n",
    "        # Feature branch\n",
    "        feature_vec = torch.cat([party_affiliation.unsqueeze(1), credibility.unsqueeze(1), sentiment.unsqueeze(1)], dim=1)\n",
    "        feature_out = F.relu(self.feature_fc(feature_vec))\n",
    "\n",
    "        # Combined branch\n",
    "        combined = torch.cat([bert_out, feature_out], dim=1)\n",
    "        print(combined.shape)\n",
    "        print(bert_out.shape)\n",
    "        combined = self.combined_fc(combined)\n",
    "\n",
    "        return F.softmax(combined, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37253eb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN version incompatibility: PyTorch was compiled  against (8, 7, 0) but found runtime version (8, 6, 0). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.Looks like your LD_LIBRARY_PATH contains incompatible version of cudnnPlease either remove it from the path or install cudnn (8, 7, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3059/3602220997.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseqBert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m# Note: be v. careful before removing this, as 3rd party device types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_flat_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_init_flat_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m         self._flat_weight_refs = [weakref.ref(w) if w is not None else None\n\u001b[1;32m    138\u001b[0m                                   for w in self._flat_weights]\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mflatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m             if (not isinstance(fw.data, Tensor) or not (fw.data.dtype == dtype) or\n\u001b[1;32m    168\u001b[0m                     \u001b[0;32mnot\u001b[0m \u001b[0mfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                     not torch.backends.cudnn.is_acceptable(fw.data)):\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/backends/cudnn/__init__.py\u001b[0m in \u001b[0;36mis_acceptable\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \"PyTorch making sure the library is visible to the build system.\")\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         warnings.warn('cuDNN/MIOpen library not found. Check your {libpath}'.format(\n\u001b[1;32m     99\u001b[0m             libpath={\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/backends/cudnn/__init__.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mld_library_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LD_LIBRARY_PATH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstring\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mld_library_path\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubstring\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cudnn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                         raise RuntimeError(f'{base_error_msg}'\n\u001b[0m\u001b[1;32m     53\u001b[0m                                            \u001b[0;34mf'Looks like your LD_LIBRARY_PATH contains incompatible version of cudnn'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                            f'Please either remove it from the path or install cudnn {compile_version}')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN version incompatibility: PyTorch was compiled  against (8, 7, 0) but found runtime version (8, 6, 0). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.Looks like your LD_LIBRARY_PATH contains incompatible version of cudnnPlease either remove it from the path or install cudnn (8, 7, 0)"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = seqBert(bert_model, 6)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bb8d92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/256 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.91 GiB total capacity; 10.16 GiB already allocated; 10.06 MiB free; 10.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2835/521171983.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         outputs = model(\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2835/528892039.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, party_affiliation, credibility, sentiment)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparty_affiliation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredibility\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# BERT branch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mbert_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# [CLS] token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mbert_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mbert_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         )\n\u001b[0;32m-> 1013\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1014\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    605\u001b[0m                 )\n\u001b[1;32m    606\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    608\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    498\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 427\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 10.91 GiB total capacity; 10.16 GiB already allocated; 10.06 MiB free; 10.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "num_epochs = 3  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_data_loader, desc=f'Epoch {epoch + 1}'):\n",
    "        for key in batch:\n",
    "            batch[key] = batch[key].to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            party_affiliation=batch['party_affiliation'],\n",
    "            credibility=batch['credibility'],\n",
    "            sentiment=batch['sentiment']\n",
    "        )\n",
    "        loss = criterion(outputs, batch['label'])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Calculate training loss\n",
    "    avg_train_loss = train_loss / len(train_data_loader)\n",
    "    print(f'Training Loss: {avg_train_loss}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_outputs = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(val_data_loader, desc=f'Epoch {epoch + 1} - Validation'):\n",
    "\n",
    "            for key in val_batch:\n",
    "                val_batch[key] = val_batch[key].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            val_output = model(\n",
    "                input_ids=val_batch['input_ids'],\n",
    "                attention_mask=val_batch['attention_mask'],\n",
    "                party_affiliation=val_batch['party_affiliation'],\n",
    "                credibility=val_batch['credibility'],\n",
    "                sentiment=val_batch['sentiment']\n",
    "            )\n",
    "            val_outputs.append(val_output)\n",
    "            val_labels.append(val_batch['label'])\n",
    "            \n",
    "        val_outputs = torch.cat(val_outputs, dim=0)\n",
    "        val_labels = torch.cat(val_labels, dim=0)\n",
    "        \n",
    "        _, predicted = torch.max(val_outputs, 1)\n",
    "        val_accuracy = accuracy_score(val_labels.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495687a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
